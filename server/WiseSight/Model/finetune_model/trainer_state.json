{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.833180568285976,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00916590284142988,
      "grad_norm": 1.8459012508392334,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 2.9072,
      "step": 10
    },
    {
      "epoch": 0.01833180568285976,
      "grad_norm": 2.506096363067627,
      "learning_rate": 4.966666666666667e-05,
      "loss": 2.7496,
      "step": 20
    },
    {
      "epoch": 0.027497708524289642,
      "grad_norm": 2.939545154571533,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 2.592,
      "step": 30
    },
    {
      "epoch": 0.03666361136571952,
      "grad_norm": 4.169103622436523,
      "learning_rate": 4.933333333333334e-05,
      "loss": 2.0886,
      "step": 40
    },
    {
      "epoch": 0.045829514207149404,
      "grad_norm": 2.9624481201171875,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 2.0041,
      "step": 50
    },
    {
      "epoch": 0.054995417048579284,
      "grad_norm": 3.180806875228882,
      "learning_rate": 4.9e-05,
      "loss": 1.939,
      "step": 60
    },
    {
      "epoch": 0.06416131989000917,
      "grad_norm": 2.1625707149505615,
      "learning_rate": 4.883333333333334e-05,
      "loss": 1.5976,
      "step": 70
    },
    {
      "epoch": 0.07332722273143905,
      "grad_norm": 2.1017563343048096,
      "learning_rate": 4.866666666666667e-05,
      "loss": 1.9419,
      "step": 80
    },
    {
      "epoch": 0.08249312557286893,
      "grad_norm": 2.1340181827545166,
      "learning_rate": 4.85e-05,
      "loss": 1.686,
      "step": 90
    },
    {
      "epoch": 0.09165902841429881,
      "grad_norm": 2.4995155334472656,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 1.6076,
      "step": 100
    },
    {
      "epoch": 0.1008249312557287,
      "grad_norm": 2.4956793785095215,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 1.7377,
      "step": 110
    },
    {
      "epoch": 0.10999083409715857,
      "grad_norm": 2.291520595550537,
      "learning_rate": 4.8e-05,
      "loss": 1.6725,
      "step": 120
    },
    {
      "epoch": 0.11915673693858846,
      "grad_norm": 2.16555118560791,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 1.7409,
      "step": 130
    },
    {
      "epoch": 0.12832263978001834,
      "grad_norm": 2.680283308029175,
      "learning_rate": 4.766666666666667e-05,
      "loss": 1.7306,
      "step": 140
    },
    {
      "epoch": 0.13748854262144822,
      "grad_norm": 2.3052330017089844,
      "learning_rate": 4.75e-05,
      "loss": 1.7159,
      "step": 150
    },
    {
      "epoch": 0.1466544454628781,
      "grad_norm": 2.663262367248535,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 1.5507,
      "step": 160
    },
    {
      "epoch": 0.15582034830430797,
      "grad_norm": 3.6019699573516846,
      "learning_rate": 4.716666666666667e-05,
      "loss": 1.4856,
      "step": 170
    },
    {
      "epoch": 0.16498625114573787,
      "grad_norm": 2.4124879837036133,
      "learning_rate": 4.7e-05,
      "loss": 1.6994,
      "step": 180
    },
    {
      "epoch": 0.17415215398716774,
      "grad_norm": 2.8824217319488525,
      "learning_rate": 4.683333333333334e-05,
      "loss": 1.4829,
      "step": 190
    },
    {
      "epoch": 0.18331805682859761,
      "grad_norm": 2.785717248916626,
      "learning_rate": 4.666666666666667e-05,
      "loss": 1.7491,
      "step": 200
    },
    {
      "epoch": 0.1924839596700275,
      "grad_norm": 3.4460854530334473,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 1.5169,
      "step": 210
    },
    {
      "epoch": 0.2016498625114574,
      "grad_norm": 5.013267993927002,
      "learning_rate": 4.633333333333333e-05,
      "loss": 1.6228,
      "step": 220
    },
    {
      "epoch": 0.21081576535288726,
      "grad_norm": 4.4406418800354,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 1.4767,
      "step": 230
    },
    {
      "epoch": 0.21998166819431714,
      "grad_norm": 3.2675235271453857,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.6686,
      "step": 240
    },
    {
      "epoch": 0.229147571035747,
      "grad_norm": 3.3764431476593018,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 1.4775,
      "step": 250
    },
    {
      "epoch": 0.2383134738771769,
      "grad_norm": 3.3758814334869385,
      "learning_rate": 4.566666666666667e-05,
      "loss": 1.3532,
      "step": 260
    },
    {
      "epoch": 0.24747937671860679,
      "grad_norm": 2.682987689971924,
      "learning_rate": 4.55e-05,
      "loss": 1.504,
      "step": 270
    },
    {
      "epoch": 0.2566452795600367,
      "grad_norm": 3.6225521564483643,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 1.3757,
      "step": 280
    },
    {
      "epoch": 0.26581118240146656,
      "grad_norm": 3.3926546573638916,
      "learning_rate": 4.516666666666667e-05,
      "loss": 1.3156,
      "step": 290
    },
    {
      "epoch": 0.27497708524289644,
      "grad_norm": 2.8145639896392822,
      "learning_rate": 4.5e-05,
      "loss": 1.496,
      "step": 300
    },
    {
      "epoch": 0.2841429880843263,
      "grad_norm": 4.7027268409729,
      "learning_rate": 4.483333333333333e-05,
      "loss": 1.494,
      "step": 310
    },
    {
      "epoch": 0.2933088909257562,
      "grad_norm": 3.901015043258667,
      "learning_rate": 4.466666666666667e-05,
      "loss": 1.4408,
      "step": 320
    },
    {
      "epoch": 0.30247479376718606,
      "grad_norm": 3.197932481765747,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.4527,
      "step": 330
    },
    {
      "epoch": 0.31164069660861593,
      "grad_norm": 4.298901557922363,
      "learning_rate": 4.433333333333334e-05,
      "loss": 1.4406,
      "step": 340
    },
    {
      "epoch": 0.3208065994500458,
      "grad_norm": 3.8607828617095947,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 1.4161,
      "step": 350
    },
    {
      "epoch": 0.32997250229147573,
      "grad_norm": 4.164295196533203,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.5229,
      "step": 360
    },
    {
      "epoch": 0.3391384051329056,
      "grad_norm": 3.604168176651001,
      "learning_rate": 4.383333333333334e-05,
      "loss": 1.4198,
      "step": 370
    },
    {
      "epoch": 0.3483043079743355,
      "grad_norm": 3.596281051635742,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 1.5118,
      "step": 380
    },
    {
      "epoch": 0.35747021081576535,
      "grad_norm": 3.3516433238983154,
      "learning_rate": 4.35e-05,
      "loss": 1.4246,
      "step": 390
    },
    {
      "epoch": 0.36663611365719523,
      "grad_norm": 3.8588104248046875,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 1.2435,
      "step": 400
    },
    {
      "epoch": 0.3758020164986251,
      "grad_norm": 3.7615299224853516,
      "learning_rate": 4.316666666666667e-05,
      "loss": 1.5141,
      "step": 410
    },
    {
      "epoch": 0.384967919340055,
      "grad_norm": 3.3231005668640137,
      "learning_rate": 4.3e-05,
      "loss": 1.3414,
      "step": 420
    },
    {
      "epoch": 0.39413382218148485,
      "grad_norm": 4.290802955627441,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 1.479,
      "step": 430
    },
    {
      "epoch": 0.4032997250229148,
      "grad_norm": 3.739586353302002,
      "learning_rate": 4.266666666666667e-05,
      "loss": 1.4408,
      "step": 440
    },
    {
      "epoch": 0.41246562786434465,
      "grad_norm": 3.539083242416382,
      "learning_rate": 4.25e-05,
      "loss": 1.3962,
      "step": 450
    },
    {
      "epoch": 0.4216315307057745,
      "grad_norm": 3.874638557434082,
      "learning_rate": 4.233333333333334e-05,
      "loss": 1.4277,
      "step": 460
    },
    {
      "epoch": 0.4307974335472044,
      "grad_norm": 3.5584661960601807,
      "learning_rate": 4.216666666666667e-05,
      "loss": 1.4581,
      "step": 470
    },
    {
      "epoch": 0.4399633363886343,
      "grad_norm": 3.7970187664031982,
      "learning_rate": 4.2e-05,
      "loss": 1.5532,
      "step": 480
    },
    {
      "epoch": 0.44912923923006415,
      "grad_norm": 3.853508710861206,
      "learning_rate": 4.183333333333334e-05,
      "loss": 1.4857,
      "step": 490
    },
    {
      "epoch": 0.458295142071494,
      "grad_norm": 4.0366010665893555,
      "learning_rate": 4.166666666666667e-05,
      "loss": 1.4785,
      "step": 500
    },
    {
      "epoch": 0.458295142071494,
      "eval_bleu-4": 0.13973276757905584,
      "eval_rouge-1": 32.921948,
      "eval_rouge-2": 12.399464,
      "eval_rouge-l": 23.939326,
      "eval_runtime": 33.2858,
      "eval_samples_per_second": 1.502,
      "eval_steps_per_second": 0.12,
      "step": 500
    },
    {
      "epoch": 0.4674610449129239,
      "grad_norm": 3.723266363143921,
      "learning_rate": 4.15e-05,
      "loss": 1.3715,
      "step": 510
    },
    {
      "epoch": 0.4766269477543538,
      "grad_norm": 5.373586654663086,
      "learning_rate": 4.133333333333333e-05,
      "loss": 1.401,
      "step": 520
    },
    {
      "epoch": 0.4857928505957837,
      "grad_norm": 3.7855188846588135,
      "learning_rate": 4.116666666666667e-05,
      "loss": 1.4915,
      "step": 530
    },
    {
      "epoch": 0.49495875343721357,
      "grad_norm": 3.904123067855835,
      "learning_rate": 4.1e-05,
      "loss": 1.3702,
      "step": 540
    },
    {
      "epoch": 0.5041246562786434,
      "grad_norm": 4.863640308380127,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 1.4704,
      "step": 550
    },
    {
      "epoch": 0.5132905591200734,
      "grad_norm": 5.729549884796143,
      "learning_rate": 4.066666666666667e-05,
      "loss": 1.4804,
      "step": 560
    },
    {
      "epoch": 0.5224564619615032,
      "grad_norm": 4.069570064544678,
      "learning_rate": 4.05e-05,
      "loss": 1.4576,
      "step": 570
    },
    {
      "epoch": 0.5316223648029331,
      "grad_norm": 5.289573669433594,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 1.4473,
      "step": 580
    },
    {
      "epoch": 0.5407882676443629,
      "grad_norm": 5.561631202697754,
      "learning_rate": 4.016666666666667e-05,
      "loss": 1.301,
      "step": 590
    },
    {
      "epoch": 0.5499541704857929,
      "grad_norm": 4.853753089904785,
      "learning_rate": 4e-05,
      "loss": 1.3744,
      "step": 600
    },
    {
      "epoch": 0.5591200733272227,
      "grad_norm": 4.369045257568359,
      "learning_rate": 3.983333333333333e-05,
      "loss": 1.3425,
      "step": 610
    },
    {
      "epoch": 0.5682859761686526,
      "grad_norm": 3.541090250015259,
      "learning_rate": 3.966666666666667e-05,
      "loss": 1.4388,
      "step": 620
    },
    {
      "epoch": 0.5774518790100825,
      "grad_norm": 3.595412492752075,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 1.3679,
      "step": 630
    },
    {
      "epoch": 0.5866177818515124,
      "grad_norm": 4.06018590927124,
      "learning_rate": 3.933333333333333e-05,
      "loss": 1.3338,
      "step": 640
    },
    {
      "epoch": 0.5957836846929423,
      "grad_norm": 3.8675458431243896,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 1.2694,
      "step": 650
    },
    {
      "epoch": 0.6049495875343721,
      "grad_norm": 4.4560651779174805,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.3716,
      "step": 660
    },
    {
      "epoch": 0.614115490375802,
      "grad_norm": 3.7568163871765137,
      "learning_rate": 3.883333333333333e-05,
      "loss": 1.474,
      "step": 670
    },
    {
      "epoch": 0.6232813932172319,
      "grad_norm": 3.4847753047943115,
      "learning_rate": 3.866666666666667e-05,
      "loss": 1.5757,
      "step": 680
    },
    {
      "epoch": 0.6324472960586618,
      "grad_norm": 4.514575004577637,
      "learning_rate": 3.85e-05,
      "loss": 1.3295,
      "step": 690
    },
    {
      "epoch": 0.6416131989000916,
      "grad_norm": 4.012134075164795,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 1.2563,
      "step": 700
    },
    {
      "epoch": 0.6507791017415215,
      "grad_norm": 4.200240135192871,
      "learning_rate": 3.816666666666667e-05,
      "loss": 1.4468,
      "step": 710
    },
    {
      "epoch": 0.6599450045829515,
      "grad_norm": 4.061552047729492,
      "learning_rate": 3.8e-05,
      "loss": 1.402,
      "step": 720
    },
    {
      "epoch": 0.6691109074243813,
      "grad_norm": 3.1119015216827393,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 1.4595,
      "step": 730
    },
    {
      "epoch": 0.6782768102658112,
      "grad_norm": 5.7630181312561035,
      "learning_rate": 3.766666666666667e-05,
      "loss": 1.356,
      "step": 740
    },
    {
      "epoch": 0.687442713107241,
      "grad_norm": 4.181892395019531,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.536,
      "step": 750
    },
    {
      "epoch": 0.696608615948671,
      "grad_norm": 4.088184833526611,
      "learning_rate": 3.733333333333334e-05,
      "loss": 1.3478,
      "step": 760
    },
    {
      "epoch": 0.7057745187901008,
      "grad_norm": 4.818606853485107,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 1.4489,
      "step": 770
    },
    {
      "epoch": 0.7149404216315307,
      "grad_norm": 4.310207843780518,
      "learning_rate": 3.7e-05,
      "loss": 1.3704,
      "step": 780
    },
    {
      "epoch": 0.7241063244729606,
      "grad_norm": 3.8518855571746826,
      "learning_rate": 3.683333333333334e-05,
      "loss": 1.4036,
      "step": 790
    },
    {
      "epoch": 0.7332722273143905,
      "grad_norm": 4.493133068084717,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 1.2873,
      "step": 800
    },
    {
      "epoch": 0.7424381301558204,
      "grad_norm": 5.645465850830078,
      "learning_rate": 3.65e-05,
      "loss": 1.2497,
      "step": 810
    },
    {
      "epoch": 0.7516040329972502,
      "grad_norm": 4.4977030754089355,
      "learning_rate": 3.633333333333333e-05,
      "loss": 1.4139,
      "step": 820
    },
    {
      "epoch": 0.7607699358386801,
      "grad_norm": 3.3916351795196533,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 1.5193,
      "step": 830
    },
    {
      "epoch": 0.76993583868011,
      "grad_norm": 4.928635597229004,
      "learning_rate": 3.6e-05,
      "loss": 1.5029,
      "step": 840
    },
    {
      "epoch": 0.7791017415215399,
      "grad_norm": 4.502391815185547,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 1.351,
      "step": 850
    },
    {
      "epoch": 0.7882676443629697,
      "grad_norm": 5.021881103515625,
      "learning_rate": 3.566666666666667e-05,
      "loss": 1.4355,
      "step": 860
    },
    {
      "epoch": 0.7974335472043996,
      "grad_norm": 4.47813081741333,
      "learning_rate": 3.55e-05,
      "loss": 1.5743,
      "step": 870
    },
    {
      "epoch": 0.8065994500458296,
      "grad_norm": 4.270227432250977,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 1.4748,
      "step": 880
    },
    {
      "epoch": 0.8157653528872594,
      "grad_norm": 5.228273391723633,
      "learning_rate": 3.516666666666667e-05,
      "loss": 1.4476,
      "step": 890
    },
    {
      "epoch": 0.8249312557286893,
      "grad_norm": 5.949326515197754,
      "learning_rate": 3.5e-05,
      "loss": 1.3194,
      "step": 900
    },
    {
      "epoch": 0.8340971585701191,
      "grad_norm": 4.186349391937256,
      "learning_rate": 3.483333333333334e-05,
      "loss": 1.2768,
      "step": 910
    },
    {
      "epoch": 0.843263061411549,
      "grad_norm": 6.107052803039551,
      "learning_rate": 3.466666666666667e-05,
      "loss": 1.4453,
      "step": 920
    },
    {
      "epoch": 0.8524289642529789,
      "grad_norm": 5.6320390701293945,
      "learning_rate": 3.45e-05,
      "loss": 1.4526,
      "step": 930
    },
    {
      "epoch": 0.8615948670944088,
      "grad_norm": 5.562510967254639,
      "learning_rate": 3.433333333333333e-05,
      "loss": 1.4477,
      "step": 940
    },
    {
      "epoch": 0.8707607699358387,
      "grad_norm": 6.54844331741333,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 1.409,
      "step": 950
    },
    {
      "epoch": 0.8799266727772685,
      "grad_norm": 4.620111465454102,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.3557,
      "step": 960
    },
    {
      "epoch": 0.8890925756186985,
      "grad_norm": 4.822589874267578,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 1.3039,
      "step": 970
    },
    {
      "epoch": 0.8982584784601283,
      "grad_norm": 4.016714096069336,
      "learning_rate": 3.366666666666667e-05,
      "loss": 1.4351,
      "step": 980
    },
    {
      "epoch": 0.9074243813015582,
      "grad_norm": 4.568165302276611,
      "learning_rate": 3.35e-05,
      "loss": 1.3483,
      "step": 990
    },
    {
      "epoch": 0.916590284142988,
      "grad_norm": 5.076407432556152,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.2764,
      "step": 1000
    },
    {
      "epoch": 0.916590284142988,
      "eval_bleu-4": 0.14811799555859978,
      "eval_rouge-1": 35.016496,
      "eval_rouge-2": 14.102666000000001,
      "eval_rouge-l": 26.049087999999998,
      "eval_runtime": 22.3002,
      "eval_samples_per_second": 2.242,
      "eval_steps_per_second": 0.179,
      "step": 1000
    },
    {
      "epoch": 0.925756186984418,
      "grad_norm": 6.1786112785339355,
      "learning_rate": 3.316666666666667e-05,
      "loss": 1.2317,
      "step": 1010
    },
    {
      "epoch": 0.9349220898258478,
      "grad_norm": 5.588381290435791,
      "learning_rate": 3.3e-05,
      "loss": 1.4286,
      "step": 1020
    },
    {
      "epoch": 0.9440879926672777,
      "grad_norm": 4.910785675048828,
      "learning_rate": 3.283333333333333e-05,
      "loss": 1.4044,
      "step": 1030
    },
    {
      "epoch": 0.9532538955087076,
      "grad_norm": 4.57017707824707,
      "learning_rate": 3.266666666666667e-05,
      "loss": 1.2445,
      "step": 1040
    },
    {
      "epoch": 0.9624197983501375,
      "grad_norm": 5.178891658782959,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 1.3767,
      "step": 1050
    },
    {
      "epoch": 0.9715857011915674,
      "grad_norm": 5.042079925537109,
      "learning_rate": 3.233333333333333e-05,
      "loss": 1.3799,
      "step": 1060
    },
    {
      "epoch": 0.9807516040329972,
      "grad_norm": 6.120806694030762,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 1.3627,
      "step": 1070
    },
    {
      "epoch": 0.9899175068744271,
      "grad_norm": 7.0357465744018555,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.3718,
      "step": 1080
    },
    {
      "epoch": 0.999083409715857,
      "grad_norm": 5.059109210968018,
      "learning_rate": 3.183333333333334e-05,
      "loss": 1.3513,
      "step": 1090
    },
    {
      "epoch": 1.008249312557287,
      "grad_norm": 3.985039234161377,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 1.3974,
      "step": 1100
    },
    {
      "epoch": 1.0174152153987168,
      "grad_norm": 3.7836785316467285,
      "learning_rate": 3.15e-05,
      "loss": 1.2928,
      "step": 1110
    },
    {
      "epoch": 1.0265811182401468,
      "grad_norm": 4.485092639923096,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 1.35,
      "step": 1120
    },
    {
      "epoch": 1.0357470210815765,
      "grad_norm": 3.904074192047119,
      "learning_rate": 3.116666666666667e-05,
      "loss": 1.2381,
      "step": 1130
    },
    {
      "epoch": 1.0449129239230064,
      "grad_norm": 4.843596935272217,
      "learning_rate": 3.1e-05,
      "loss": 1.2148,
      "step": 1140
    },
    {
      "epoch": 1.0540788267644363,
      "grad_norm": 3.964064359664917,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 1.3457,
      "step": 1150
    },
    {
      "epoch": 1.0632447296058662,
      "grad_norm": 6.689198017120361,
      "learning_rate": 3.066666666666667e-05,
      "loss": 1.4036,
      "step": 1160
    },
    {
      "epoch": 1.072410632447296,
      "grad_norm": 4.224462985992432,
      "learning_rate": 3.05e-05,
      "loss": 1.2839,
      "step": 1170
    },
    {
      "epoch": 1.0815765352887259,
      "grad_norm": 5.384086608886719,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 1.1916,
      "step": 1180
    },
    {
      "epoch": 1.0907424381301558,
      "grad_norm": 4.67611026763916,
      "learning_rate": 3.016666666666667e-05,
      "loss": 1.2235,
      "step": 1190
    },
    {
      "epoch": 1.0999083409715857,
      "grad_norm": 5.754863262176514,
      "learning_rate": 3e-05,
      "loss": 1.3295,
      "step": 1200
    },
    {
      "epoch": 1.1090742438130157,
      "grad_norm": 6.569850921630859,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 1.3619,
      "step": 1210
    },
    {
      "epoch": 1.1182401466544454,
      "grad_norm": 5.39099645614624,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 1.2892,
      "step": 1220
    },
    {
      "epoch": 1.1274060494958753,
      "grad_norm": 4.8459296226501465,
      "learning_rate": 2.95e-05,
      "loss": 1.3846,
      "step": 1230
    },
    {
      "epoch": 1.1365719523373052,
      "grad_norm": 5.155452728271484,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 1.2837,
      "step": 1240
    },
    {
      "epoch": 1.1457378551787352,
      "grad_norm": 6.329339981079102,
      "learning_rate": 2.916666666666667e-05,
      "loss": 1.1442,
      "step": 1250
    },
    {
      "epoch": 1.1549037580201649,
      "grad_norm": 4.911947250366211,
      "learning_rate": 2.9e-05,
      "loss": 1.2678,
      "step": 1260
    },
    {
      "epoch": 1.1640696608615948,
      "grad_norm": 5.487979412078857,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 1.2079,
      "step": 1270
    },
    {
      "epoch": 1.1732355637030247,
      "grad_norm": 4.911664962768555,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 1.2832,
      "step": 1280
    },
    {
      "epoch": 1.1824014665444547,
      "grad_norm": 5.359835624694824,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.331,
      "step": 1290
    },
    {
      "epoch": 1.1915673693858846,
      "grad_norm": 5.124743461608887,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 1.1829,
      "step": 1300
    },
    {
      "epoch": 1.2007332722273143,
      "grad_norm": 5.343438148498535,
      "learning_rate": 2.816666666666667e-05,
      "loss": 1.3895,
      "step": 1310
    },
    {
      "epoch": 1.2098991750687442,
      "grad_norm": 4.070722579956055,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.357,
      "step": 1320
    },
    {
      "epoch": 1.2190650779101742,
      "grad_norm": 9.33734130859375,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 1.3591,
      "step": 1330
    },
    {
      "epoch": 1.228230980751604,
      "grad_norm": 4.543191909790039,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 1.3938,
      "step": 1340
    },
    {
      "epoch": 1.2373968835930338,
      "grad_norm": 4.2223429679870605,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 1.389,
      "step": 1350
    },
    {
      "epoch": 1.2465627864344637,
      "grad_norm": 4.74168062210083,
      "learning_rate": 2.733333333333333e-05,
      "loss": 1.2675,
      "step": 1360
    },
    {
      "epoch": 1.2557286892758937,
      "grad_norm": 5.368317604064941,
      "learning_rate": 2.716666666666667e-05,
      "loss": 1.3403,
      "step": 1370
    },
    {
      "epoch": 1.2648945921173236,
      "grad_norm": 5.628664016723633,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.2954,
      "step": 1380
    },
    {
      "epoch": 1.2740604949587535,
      "grad_norm": 4.953526973724365,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 1.3423,
      "step": 1390
    },
    {
      "epoch": 1.2832263978001834,
      "grad_norm": 5.345575332641602,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 1.3728,
      "step": 1400
    },
    {
      "epoch": 1.2923923006416131,
      "grad_norm": 5.006406784057617,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 1.2665,
      "step": 1410
    },
    {
      "epoch": 1.301558203483043,
      "grad_norm": 7.36677885055542,
      "learning_rate": 2.633333333333333e-05,
      "loss": 1.2639,
      "step": 1420
    },
    {
      "epoch": 1.310724106324473,
      "grad_norm": 6.327718734741211,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 1.371,
      "step": 1430
    },
    {
      "epoch": 1.3198900091659027,
      "grad_norm": 5.427948474884033,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.2895,
      "step": 1440
    },
    {
      "epoch": 1.3290559120073326,
      "grad_norm": 5.8786468505859375,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 1.3713,
      "step": 1450
    },
    {
      "epoch": 1.3382218148487626,
      "grad_norm": 6.8157854080200195,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 1.2779,
      "step": 1460
    },
    {
      "epoch": 1.3473877176901925,
      "grad_norm": 4.394193172454834,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 1.2255,
      "step": 1470
    },
    {
      "epoch": 1.3565536205316224,
      "grad_norm": 4.660518646240234,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 1.24,
      "step": 1480
    },
    {
      "epoch": 1.3657195233730524,
      "grad_norm": 5.6616668701171875,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 1.2969,
      "step": 1490
    },
    {
      "epoch": 1.374885426214482,
      "grad_norm": 6.201033115386963,
      "learning_rate": 2.5e-05,
      "loss": 1.2873,
      "step": 1500
    },
    {
      "epoch": 1.374885426214482,
      "eval_bleu-4": 0.16222503234594954,
      "eval_rouge-1": 35.563638000000005,
      "eval_rouge-2": 14.972208,
      "eval_rouge-l": 27.611454,
      "eval_runtime": 10.8278,
      "eval_samples_per_second": 4.618,
      "eval_steps_per_second": 0.369,
      "step": 1500
    },
    {
      "epoch": 1.384051329055912,
      "grad_norm": 4.165238380432129,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 1.2961,
      "step": 1510
    },
    {
      "epoch": 1.393217231897342,
      "grad_norm": 4.641732692718506,
      "learning_rate": 2.466666666666667e-05,
      "loss": 1.3028,
      "step": 1520
    },
    {
      "epoch": 1.4023831347387716,
      "grad_norm": 5.135631084442139,
      "learning_rate": 2.45e-05,
      "loss": 1.297,
      "step": 1530
    },
    {
      "epoch": 1.4115490375802016,
      "grad_norm": 5.224415302276611,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 1.3703,
      "step": 1540
    },
    {
      "epoch": 1.4207149404216315,
      "grad_norm": 5.634960651397705,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 1.2441,
      "step": 1550
    },
    {
      "epoch": 1.4298808432630614,
      "grad_norm": 5.327917575836182,
      "learning_rate": 2.4e-05,
      "loss": 1.308,
      "step": 1560
    },
    {
      "epoch": 1.4390467461044913,
      "grad_norm": 5.2693095207214355,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 1.2574,
      "step": 1570
    },
    {
      "epoch": 1.4482126489459213,
      "grad_norm": 4.8647308349609375,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 1.3402,
      "step": 1580
    },
    {
      "epoch": 1.457378551787351,
      "grad_norm": 5.640548229217529,
      "learning_rate": 2.35e-05,
      "loss": 1.1578,
      "step": 1590
    },
    {
      "epoch": 1.466544454628781,
      "grad_norm": 5.324955463409424,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 1.2505,
      "step": 1600
    },
    {
      "epoch": 1.4757103574702108,
      "grad_norm": 7.699394702911377,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 1.2873,
      "step": 1610
    },
    {
      "epoch": 1.4848762603116408,
      "grad_norm": 4.461190223693848,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.2422,
      "step": 1620
    },
    {
      "epoch": 1.4940421631530705,
      "grad_norm": 4.797999382019043,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 1.3931,
      "step": 1630
    },
    {
      "epoch": 1.5032080659945004,
      "grad_norm": 5.779721736907959,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 1.3021,
      "step": 1640
    },
    {
      "epoch": 1.5123739688359303,
      "grad_norm": 4.9512224197387695,
      "learning_rate": 2.25e-05,
      "loss": 1.2875,
      "step": 1650
    },
    {
      "epoch": 1.5215398716773603,
      "grad_norm": 5.257732391357422,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 1.3664,
      "step": 1660
    },
    {
      "epoch": 1.5307057745187902,
      "grad_norm": 4.987130641937256,
      "learning_rate": 2.216666666666667e-05,
      "loss": 1.1742,
      "step": 1670
    },
    {
      "epoch": 1.5398716773602201,
      "grad_norm": 4.29194450378418,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.4092,
      "step": 1680
    },
    {
      "epoch": 1.5490375802016498,
      "grad_norm": 4.936371803283691,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 1.2225,
      "step": 1690
    },
    {
      "epoch": 1.5582034830430798,
      "grad_norm": 6.726705074310303,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 1.2888,
      "step": 1700
    },
    {
      "epoch": 1.5673693858845095,
      "grad_norm": 5.674614906311035,
      "learning_rate": 2.15e-05,
      "loss": 1.2554,
      "step": 1710
    },
    {
      "epoch": 1.5765352887259394,
      "grad_norm": 6.034246444702148,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 1.2708,
      "step": 1720
    },
    {
      "epoch": 1.5857011915673693,
      "grad_norm": 5.185705184936523,
      "learning_rate": 2.116666666666667e-05,
      "loss": 1.3471,
      "step": 1730
    },
    {
      "epoch": 1.5948670944087993,
      "grad_norm": 4.899049282073975,
      "learning_rate": 2.1e-05,
      "loss": 1.2601,
      "step": 1740
    },
    {
      "epoch": 1.6040329972502292,
      "grad_norm": 5.2260422706604,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 1.353,
      "step": 1750
    },
    {
      "epoch": 1.6131989000916591,
      "grad_norm": 5.487934112548828,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 1.1249,
      "step": 1760
    },
    {
      "epoch": 1.622364802933089,
      "grad_norm": 4.399502277374268,
      "learning_rate": 2.05e-05,
      "loss": 1.3114,
      "step": 1770
    },
    {
      "epoch": 1.6315307057745188,
      "grad_norm": 5.571034908294678,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 1.3988,
      "step": 1780
    },
    {
      "epoch": 1.6406966086159487,
      "grad_norm": 6.184436798095703,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 1.2751,
      "step": 1790
    },
    {
      "epoch": 1.6498625114573784,
      "grad_norm": 4.543420791625977,
      "learning_rate": 2e-05,
      "loss": 1.3201,
      "step": 1800
    },
    {
      "epoch": 1.6590284142988083,
      "grad_norm": 4.868380546569824,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 1.1787,
      "step": 1810
    },
    {
      "epoch": 1.6681943171402382,
      "grad_norm": 6.231926918029785,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 1.414,
      "step": 1820
    },
    {
      "epoch": 1.6773602199816682,
      "grad_norm": 5.110104084014893,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 1.362,
      "step": 1830
    },
    {
      "epoch": 1.686526122823098,
      "grad_norm": 5.873159408569336,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 1.257,
      "step": 1840
    },
    {
      "epoch": 1.695692025664528,
      "grad_norm": 7.149530410766602,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 1.3295,
      "step": 1850
    },
    {
      "epoch": 1.704857928505958,
      "grad_norm": 5.436850547790527,
      "learning_rate": 1.9e-05,
      "loss": 1.3671,
      "step": 1860
    },
    {
      "epoch": 1.7140238313473877,
      "grad_norm": 5.186648368835449,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 1.2849,
      "step": 1870
    },
    {
      "epoch": 1.7231897341888176,
      "grad_norm": 5.297126293182373,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.3122,
      "step": 1880
    },
    {
      "epoch": 1.7323556370302475,
      "grad_norm": 4.354101181030273,
      "learning_rate": 1.85e-05,
      "loss": 1.3168,
      "step": 1890
    },
    {
      "epoch": 1.7415215398716772,
      "grad_norm": 6.398829936981201,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 1.203,
      "step": 1900
    },
    {
      "epoch": 1.7506874427131072,
      "grad_norm": 4.972517967224121,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 1.2794,
      "step": 1910
    },
    {
      "epoch": 1.759853345554537,
      "grad_norm": 5.4775919914245605,
      "learning_rate": 1.8e-05,
      "loss": 1.2653,
      "step": 1920
    },
    {
      "epoch": 1.769019248395967,
      "grad_norm": 6.651690483093262,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 1.2011,
      "step": 1930
    },
    {
      "epoch": 1.778185151237397,
      "grad_norm": 7.077657222747803,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 1.3997,
      "step": 1940
    },
    {
      "epoch": 1.7873510540788269,
      "grad_norm": 4.851014137268066,
      "learning_rate": 1.75e-05,
      "loss": 1.2537,
      "step": 1950
    },
    {
      "epoch": 1.7965169569202566,
      "grad_norm": 4.9735941886901855,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 1.2381,
      "step": 1960
    },
    {
      "epoch": 1.8056828597616865,
      "grad_norm": 5.793521404266357,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 1.2081,
      "step": 1970
    },
    {
      "epoch": 1.8148487626031164,
      "grad_norm": 6.846750259399414,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.3155,
      "step": 1980
    },
    {
      "epoch": 1.8240146654445462,
      "grad_norm": 5.235828399658203,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 1.2262,
      "step": 1990
    },
    {
      "epoch": 1.833180568285976,
      "grad_norm": 5.183424472808838,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.2698,
      "step": 2000
    },
    {
      "epoch": 1.833180568285976,
      "eval_bleu-4": 0.1352739347921364,
      "eval_rouge-1": 34.347851999999996,
      "eval_rouge-2": 13.832486000000001,
      "eval_rouge-l": 25.091987999999997,
      "eval_runtime": 20.3868,
      "eval_samples_per_second": 2.453,
      "eval_steps_per_second": 0.196,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 2000,
  "total_flos": 4.608140775993754e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
